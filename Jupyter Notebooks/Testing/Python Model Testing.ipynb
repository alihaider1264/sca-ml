{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-28 14:26:00.040174: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-28 14:26:01.402729: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-28 14:26:01.402812: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-28 14:26:01.402819: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "pathToModels = \"/home/mcall/models/\"\n",
    "pathToTests = \"/home/mcall/modelTest/\"\n",
    "\"\"\"Startup\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tkinter import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from tokenize import TokenInfo\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import CuDNNLSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Conv2D\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "slashForDir = \"/\"\n",
    "def convSlashes(path):\n",
    "    return path.replace(\"/\", slashForDir).replace(\"\\\\\", slashForDir)\n",
    "\n",
    "\n",
    "# Print iterations progress\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = 'â–ˆ', printEnd = \"\\r\"):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()\n",
    "        \n",
    "def searchFiles(path, fileformats):\n",
    "    filesToDo = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            for fileformat in fileformats:\n",
    "                if file.endswith(fileformat):\n",
    "                    #code to generate a list of paths of files to generate logs for\n",
    "                    filesToDo.append(os.path.join(root, file).removeprefix(path))\n",
    "    return filesToDo\n",
    "\n",
    "def searchFileName(path, fileName):\n",
    "    filesToDo = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file == fileName:\n",
    "                #code to generate a list of paths of files to generate logs for\n",
    "                filesToDo.append(os.path.join(root, file).removeprefix(path))\n",
    "    return filesToDo\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Text Processing\"\n",
    "\"Up to date with model creation since 11/25/22\"\n",
    "\n",
    "\n",
    "regexBlankLineMostly = r\"(^\\s{1,}$)\"\n",
    "regexBlankLineFinish = r\"(^\\n{1,})\"\n",
    "\n",
    "def stringProcessing(string):\n",
    "    string = string.replace(\"\\\"\\\"\\\"\" , \"\\\"\")\n",
    "    string = string.replace(\"\\'\\'\\'\" , \"\\\"\")\n",
    "    string = string.replace(\"\\\\'\" , \"\")\n",
    "    string = string.replace('\\\\\"' , \"\")\n",
    "    string = re.sub(\"('|\\\")[\\x1f-\\x7e]{1,}?('|\\\")\", \" \\\"sGH\\\"\", string)\n",
    "    string = re.sub(\"#.*\", \"\", string)\n",
    "    string = string.replace('    ', '\\t')\n",
    "    string = re.sub(regexBlankLineMostly, '', string, 0, re.MULTILINE)\n",
    "    string = re.sub(regexBlankLineFinish, '', string, 0, re.MULTILINE)\n",
    "    return string\n",
    "\n",
    "\n",
    "to_pad = ['\\n', '\\t', '\\r', '(', ')', '[', ']', '{', '}', '<', '>', '!', '?', ',', '.', ':', ';', '`', '~', '@', '#', '$', '%', '^', '&', '*', '=', '+', '/', '\\\\', '|']\n",
    "\n",
    "def stringPadding(dataSet):\n",
    "    if (type(dataSet) == str):\n",
    "        for i in range(len(to_pad)):\n",
    "            dataSet = dataSet.replace(to_pad[i], ' ' + to_pad[i] + ' ')\n",
    "        return dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-28 14:26:15.666237: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-28 14:26:15.692460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-28 14:26:15.692682: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-28 14:26:15.693763: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-28 14:26:15.695936: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-28 14:26:15.696131: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-28 14:26:15.696286: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-28 14:26:18.359006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-28 14:26:18.359534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-28 14:26:18.359545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-11-28 14:26:18.359742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-28 14:26:18.360009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4936 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 13117, 64)         96064     \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 13117, 128)       66560     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 128)              99328     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,113\n",
      "Trainable params: 266,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#load 1 model\n",
    "modelName = \"python_model_1_RNN\"\n",
    "\n",
    "model = load_model(pathToModels + modelName + slashForDir + 'model.h5')\n",
    "\n",
    "model.summary()\n",
    "import pickle\n",
    "with open(pathToModels + modelName + slashForDir + 'tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "with open(pathToModels + modelName + slashForDir + 'modelInfo.json') as json_file:\n",
    "    modelInfo = json.load(json_file)\n",
    "    modelInfo = {**modelInfo, **{\"modelName\": modelName}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-28 14:23:07.449666: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-28 14:23:07.449923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-28 14:23:07.450135: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-28 14:23:07.452192: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-28 14:23:07.452366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-28 14:23:07.452520: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-28 14:23:07.454484: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-28 14:23:07.454499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-11-28 14:23:07.454672: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-28 14:23:07.454701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4936 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2022-11-28 14:23:11.621722: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1309933568 exceeds 10% of free system memory.\n",
      "2022-11-28 14:23:12.395918: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1309933568 exceeds 10% of free system memory.\n",
      "2022-11-28 14:23:13.161893: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1309933568 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"load Array of models\"\n",
    "modelNames = [\"python_model_1_RNN\", \"python_model_1_IMPROVED\"]\n",
    "models = []\n",
    "tokenizers = []\n",
    "modelInfos = []\n",
    "for modelName in modelNames:\n",
    "    model = load_model(pathToModels + modelName + '/model.h5')\n",
    "    models.append(model)\n",
    "    with open(pathToModels + modelName + '/tokenizer.pickle', 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "        tokenizers.append(tokenizer)\n",
    "    with open(pathToModels + modelName + '/modelInfo.json') as json_file:\n",
    "        modelInfo = json.load(json_file)\n",
    "        #add the model name\n",
    "        modelInfo = {**modelInfo, **{\"modelName\": modelName}}\n",
    "        modelInfos.append(modelInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Test Models\"\n",
    "\n",
    "\"\"\"Folder definition for Model Testing:\n",
    "FOLDER:\n",
    "testInfo.json (Contains the type of test and information for each file. See below)\n",
    "testCode\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Types of test: \n",
    "Pos Linear comparative (PosLinCar): Each sequential file is expected to score higher than the last. Files are named 0.txt, 1.txt, 2.txt, etc.\n",
    "Neutral (Neutral): Each file is expected to score about the same. Files are named 0.txt, 1.txt, 2.txt, etc.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def modelTesting(modell, tokenizerl, modelInfol, testFolder): \n",
    "    with open(testFolder + slashForDir + 'testInfo.json') as json_file:\n",
    "        testInfo = json.load(json_file)\n",
    "    #Check if model is an array of models\n",
    "    if (type(modell) == list):\n",
    "        for i in range(len(modell)):\n",
    "            modelTestingRunner(modell[i], tokenizerl[i], modelInfol[i], testFolder, testInfo)\n",
    "    else:\n",
    "        modelTestingRunner(modell, tokenizerl, modelInfol, testFolder, testInfo)\n",
    "        \n",
    "def modelTestingRunner(modell, tokenizerl, modelInfol, testFolder, testInfo):\n",
    "    print (\"Testing \" + modelInfol[\"modelName\"])\n",
    "    if testInfo['type'] == 'LinCar':\n",
    "        posLinCarTest(modell, tokenizerl, modelInfol, testFolder)\n",
    "    if testInfo['type'] == 'Neutral':\n",
    "        neutralTest(modell, tokenizerl, modelInfol, testFolder)\n",
    "    #Check if model is an array of models\n",
    "\n",
    "    \n",
    "def posLinCarTest(modell, tokenizerl, modelInfo1, testFolder):\n",
    "    #load testInfo.json\n",
    "    scores = []\n",
    "    #load testCode\n",
    "    filesToLoad = searchFiles(testFolder, ['.py'])\n",
    "    for file in filesToLoad:\n",
    "        with open(testFolder + file) as f:\n",
    "            code = f.read()\n",
    "            code = stringProcessing(code)\n",
    "            code = stringPadding(code)\n",
    "            code = tokenizerl.texts_to_sequences([code])\n",
    "            code = pad_sequences(code, maxlen=modelInfo1['maxLen'])\n",
    "            score = modell.predict(code)\n",
    "            scores = np.append(scores, score)\n",
    "            \n",
    "    scoresDelta = []\n",
    "    for i in range(len(scores) - 1):\n",
    "        scoresDelta = np.append(scoresDelta, scores[i + 1] - scores[i])\n",
    "    print (scores)\n",
    "    print (scoresDelta)\n",
    "    print (\"expected outcome, scores should go up as the files go up, scoresDeltas should be positive\")\n",
    "    #graph scores in a scatter plot \n",
    "    plt.scatter(range(len(scores)), scores, label=modelInfo1['modelName'])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.scatter(range(len(scoresDelta)), scoresDelta, label=modelInfo1['modelName']+ \" Delta\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def neutralTest(modell, tokenizerl, modelInfo1, testFolder):\n",
    "    scores = []\n",
    "    #load testCode\n",
    "    filesToLoad = searchFiles(testFolder, ['.py'])\n",
    "#sort the files\n",
    "    for file in range(len(filesToLoad)):\n",
    "        with open(testFolder + slashForDir + str(file) + \".py\") as f:\n",
    "            code = f.read()\n",
    "            code = stringProcessing(code)\n",
    "            print (code)\n",
    "            code = stringPadding(code)\n",
    "            print (code)\n",
    "            code = tokenizerl.texts_to_sequences([code])\n",
    "            print (\"padded\")\n",
    "            print (code)\n",
    "            print (\"revesed\")\n",
    "            print (tokenizerl.sequences_to_texts(code))\n",
    "            code = pad_sequences(code, maxlen=modelInfo1['maxLen'])\n",
    "            score = modell.predict(code)\n",
    "            scores = np.append(scores, score)\n",
    "            break\n",
    "            \n",
    "    scoresDelta = []\n",
    "    for i in range(len(scores) - 1):\n",
    "        scoresDelta = np.append(scoresDelta, scores[i + 1] - scores[i])\n",
    "    print (scores)\n",
    "    print (scoresDelta)\n",
    "    print (\"Biggest Score Delta: \" + str(max(scoresDelta)))\n",
    "    print (\"expected outcome, scores should be about the same, scoresDeltas should be about the same\")\n",
    "    #graph scores in a scatter plot, label the score the model name\n",
    "    plt.scatter(range(len(scores)), scores, label=modelInfo1['modelName'])\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    plt.scatter(range(len(scoresDelta)), scoresDelta, label=modelInfo1['modelName']+ \" Delta\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'maxWords': 1500, 'maxLen': 13117, 'modelName': 'python_model_1_RNN'}\n",
      "Testing python_model_1_RNN\n",
      "def freq(arr):\n",
      "\tcount = 0;\n",
      "\tarr.sort()\n",
      "\tfor i in range(len(arr)):\n",
      "\t\tif(arr[i] == arr[(i+1)%len(arr)]):\n",
      "\t\t\tcount += 1;\n",
      "\t\telse:\n",
      "\t\t\tprint(arr[i]+ \"sGH\"+str(count+1))\n",
      "\t\t\tcount = 0\n",
      "stg = input( \"sGH\"))\n",
      "arr = [e for e in stg]\n",
      "print( \"sGH\")\n",
      "freq(arr)\n",
      "\n",
      "def freq ( arr )  :  \n",
      "  \t count  =  0 ;  \n",
      "  \t arr . sort (  )  \n",
      "  \t for i in range ( len ( arr )  )  :  \n",
      "  \t  \t if ( arr [ i ]   =  =  arr [  ( i + 1 )  % len ( arr )  ]  )  :  \n",
      "  \t  \t  \t count  +  =  1 ;  \n",
      "  \t  \t else :  \n",
      "  \t  \t  \t print ( arr [ i ]  +  \"sGH\" + str ( count + 1 )  )  \n",
      "  \t  \t  \t count  =  0 \n",
      " stg  =  input (  \"sGH\" )  )  \n",
      " arr  =   [ e for e in stg ]  \n",
      " print (  \"sGH\" )  \n",
      " freq ( arr )  \n",
      " \n",
      "padded\n",
      "[[14, 5, 4, 8, 2, 1, 264, 7, 24, 156, 2, 1, 3, 957, 5, 4, 2, 1, 17, 63, 16, 104, 5, 59, 5, 4, 4, 8, 2, 1, 1, 13, 5, 12, 63, 11, 7, 7, 12, 5, 63, 42, 25, 4, 30, 59, 5, 4, 11, 4, 8, 2, 1, 1, 1, 264, 42, 7, 25, 156, 2, 1, 1, 40, 8, 2, 1, 1, 1, 140, 5, 12, 63, 11, 42, 9, 42, 38, 5, 264, 42, 25, 4, 4, 2, 1, 1, 1, 264, 7, 24, 2, 7, 361, 5, 9, 4, 4, 2, 7, 12, 50, 17, 50, 16, 11, 2, 140, 5, 9, 4, 2, 5, 4, 2]]\n",
      "revesed\n",
      "['def ( ) : \\n \\t count = 0 ; \\n \\t . sort ( ) \\n \\t for i in range ( len ( ) ) : \\n \\t \\t if ( [ i ] = = [ ( i + 1 ) % len ( ) ] ) : \\n \\t \\t \\t count + = 1 ; \\n \\t \\t else : \\n \\t \\t \\t print ( [ i ] + \"sgh\" + str ( count + 1 ) ) \\n \\t \\t \\t count = 0 \\n = input ( \"sgh\" ) ) \\n = [ e for e in ] \\n print ( \"sgh\" ) \\n ( ) \\n']\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "[49.99940491]\n",
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mtemp testing stuff\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m (modelInfo)\n\u001b[0;32m----> 6\u001b[0m modelTesting(model, tokenizer, modelInfo, pathToTests \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mASS-OCC\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [20], line 25\u001b[0m, in \u001b[0;36mmodelTesting\u001b[0;34m(modell, tokenizerl, modelInfol, testFolder)\u001b[0m\n\u001b[1;32m     23\u001b[0m         modelTestingRunner(modell[i], tokenizerl[i], modelInfol[i], testFolder, testInfo)\n\u001b[1;32m     24\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     modelTestingRunner(modell, tokenizerl, modelInfol, testFolder, testInfo)\n",
      "Cell \u001b[0;32mIn [20], line 32\u001b[0m, in \u001b[0;36mmodelTestingRunner\u001b[0;34m(modell, tokenizerl, modelInfol, testFolder, testInfo)\u001b[0m\n\u001b[1;32m     30\u001b[0m     posLinCarTest(modell, tokenizerl, modelInfol, testFolder)\n\u001b[1;32m     31\u001b[0m \u001b[39mif\u001b[39;00m testInfo[\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mNeutral\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 32\u001b[0m     neutralTest(modell, tokenizerl, modelInfol, testFolder)\n",
      "Cell \u001b[0;32mIn [20], line 93\u001b[0m, in \u001b[0;36mneutralTest\u001b[0;34m(modell, tokenizerl, modelInfo1, testFolder)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mprint\u001b[39m (scores)\n\u001b[1;32m     92\u001b[0m \u001b[39mprint\u001b[39m (scoresDelta)\n\u001b[0;32m---> 93\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mBiggest Score Delta: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mmax\u001b[39;49m(scoresDelta)))\n\u001b[1;32m     94\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mexpected outcome, scores should be about the same, scoresDeltas should be about the same\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[39m#graph scores in a scatter plot, label the score the model name\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "\"temp testing stuff\"\n",
    "print (modelInfo)\n",
    "\n",
    "\n",
    "\n",
    "modelTesting(model, tokenizer, modelInfo, pathToTests + 'ASS-OCC')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_445/190225126.py:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-28 14:12:38.567587: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-28 14:12:38.709773: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-28 14:12:38.736127: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-28 14:12:38.736365: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-28 14:12:41.709980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-28 14:12:41.710562: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-28 14:12:41.710577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-11-28 14:12:41.710780: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-11-28 14:12:41.711123: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:0 with 4936 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "#test to make sure gpu is detected\n",
    "tensorflow.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f46f759108ef8152d2753a0a3a4a884a2a3b25b301993d13cb15ef93c8cacead"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
